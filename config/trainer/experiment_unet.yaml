# Experiment: UNet with strided conv downsamples and compact ResNet bottleneck

_base_: "base.yaml"

run_name: "monet-unet-compact128-resnet9"
description: "UNet with strided conv downsamples, 128-channel bottleneck, 9 residual blocks, full skips, linear output"

s3:
  prefix: "monet-gan/models/unet-compact128-resnet9"

wandb:
  tags:
    - "cyclegan"
    - "unet"
    - "strided-conv"
    - "compact128"
    - "resnet9"
    - "instancenorm"

generator:
  encoder:
    # Level 0: 64x64
    - type: conv
      in_channels: 3
      out_channels: 64
      kernel_size: 7
      stride: 1
      padding: 3
      padding_mode: reflect
      norm: instance
      activation: relu
    
    # Level 1: 64x64 -> 32x32
    - type: conv
      in_channels: 64
      out_channels: 128
      kernel_size: 3
      stride: 2
      padding: 1
      padding_mode: reflect
      norm: instance
      activation: relu
    
    # Level 2: 32x32 -> 16x16
    - type: conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      stride: 2
      padding: 1
      padding_mode: reflect
      norm: instance
      activation: relu
    
    # Level 3: 16x16 -> 8x8
    - type: conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      stride: 2
      padding: 1
      padding_mode: reflect
      norm: instance
      activation: relu
    
    # Level 4: 8x8 -> 4x4
    - type: conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      stride: 2
      padding: 1
      padding_mode: reflect
      norm: instance
      activation: relu
  
  bottleneck:
    # 4x4 resolution, 128 channels
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
  
  decoder:
    # Level 3: 4x4 -> 8x8, concat with encoder[3] (128ch) -> 256ch input
    - type: upsample_conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      padding_mode: reflect
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 256
      out_channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    
    # Level 2: 8x8 -> 16x16 (no skip here)
    - type: upsample_conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      padding_mode: reflect
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    
    # Level 1: 16x16 -> 32x32 (no skip here)
    - type: upsample_conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      padding_mode: reflect
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
    
    # Level 0: 32x32 -> 64x64 (no skip here)
    - type: upsample_conv
      in_channels: 128
      out_channels: 64
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      padding_mode: reflect
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 64
      out_channels: 32
      kernel_size: 3
      padding_mode: reflect
      norm: instance
      activation: relu
      dropout: 0.0
  
  output:
    - type: conv
      in_channels: 32
      out_channels: 3
      kernel_size: 1
      stride: 1
      padding: 0
      norm: none
      activation: none
  
  # Skip connections: [encoder_block_idx, decoder_block_idx]
  # Active: encoder[3] = 8x8 -> decoder[1]
  # Optional (disabled): encoder[2] -> decoder[3], encoder[1] -> decoder[5], encoder[0] -> decoder[7]
  skip_connections:
    - [3, 1]
    # - [2, 3]
    # - [1, 5]
    # - [0, 7]

discriminator:
  type: patchgan
  input_channels: 3
  base_channels: 64
  num_layers: 3
  kernel_size: 4
  norm: instance
  activation: leaky_relu
  use_sigmoid: false

training:
  num_epochs: 200
  batch_size: 1
  num_workers: 4
  buffer_size: 50
  save_interval: 5
  log_interval: 1
  image_log_interval: 200
  num_sample_images: 4
  normalize:
    mean: [0.5, 0.5, 0.5]
    std: [0.5, 0.5, 0.5]

optimizer:
  lr: 0.0002
  betas: [0.5, 0.999]

scheduler:
  wait_epochs: 100
  decay_epochs: 100
  min_lr: 0.0

losses:
  lambda_cycle: 10.0
  lambda_identity: 5.0
