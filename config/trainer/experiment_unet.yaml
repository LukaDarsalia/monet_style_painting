# Experiment: UNet-style generator with a single deep skip connection and extra encoder residual blocks

_base_: "base.yaml"

run_name: "monet-unet-deep-skip"
description: "UNet-style generator with one deep skip connection and extra encoder residual blocks"

s3:
  prefix: "monet-gan/models/unet-deep-skip"

wandb:
  tags:
    - "cyclegan"
    - "unet"
    - "deep-skip"
    - "instancenorm"

generator:
  encoder:
    # Level 0: 64x64 -> 32x32
    - type: double_conv
      in_channels: 3
      out_channels: 64
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 64
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 64
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: max_pool
      kernel_size: 2
    
    # Level 1: 32x32 -> 16x16
    - type: double_conv
      in_channels: 64
      out_channels: 128
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: max_pool
      kernel_size: 2
    
    # Level 2: 16x16 -> 8x8
    - type: double_conv
      in_channels: 128
      out_channels: 256
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 256
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 256
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: max_pool
      kernel_size: 2
    
    # Level 3: 8x8 -> 4x4
    - type: double_conv
      in_channels: 256
      out_channels: 512
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 512
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 512
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: max_pool
      kernel_size: 2
  
  bottleneck:
    # 4x4 resolution
    - type: double_conv
      in_channels: 512
      out_channels: 1024
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 1024
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
  
  decoder:
    # Level 3: 4x4 -> 8x8, concat with encoder[10] (512ch) -> 1024ch input
    - type: upsample_conv
      in_channels: 1024
      out_channels: 512
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 1024
      out_channels: 512
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 512
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    
    # Level 2: 8x8 -> 16x16
    - type: upsample_conv
      in_channels: 512
      out_channels: 256
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 256
      out_channels: 256
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 256
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    
    # Level 1: 16x16 -> 32x32
    - type: upsample_conv
      in_channels: 256
      out_channels: 128
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 128
      out_channels: 128
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 128
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    
    # Level 0: 32x32 -> 64x64
    - type: upsample_conv
      in_channels: 128
      out_channels: 64
      kernel_size: 3
      scale_factor: 2
      mode: bilinear
      norm: instance
      activation: relu
    - type: double_conv
      in_channels: 64
      out_channels: 64
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
    - type: residual
      channels: 64
      kernel_size: 3
      norm: instance
      activation: relu
      dropout: 0.0
  
  output:
    - type: conv
      in_channels: 64
      out_channels: 3
      kernel_size: 1
      stride: 1
      padding: 0
      norm: none
      activation: tanh
  
  # Skip connections: [encoder_block_idx, decoder_block_idx]
  # encoder[14] = 8x8 (deep skip before last pool) -> decoder[1]
  skip_connections:
    - [14, 1]

discriminator:
  type: patchgan
  input_channels: 3
  base_channels: 64
  num_layers: 3
  kernel_size: 4
  norm: instance
  activation: leaky_relu
  use_sigmoid: false

optimizer:
  generator:
    type: adam
    lr: 0.0001
    betas: [0.0, 0.9]
  
  discriminator:
    type: adam
    lr: 0.0001
    betas: [0.0, 0.9]

losses:
  gan_mode: wgan-gp
  cycle_loss_type: l1
  identity_loss_type: l1
  
  weights:
    adversarial: 1.0
    cycle: 10.0
    identity: 5.0
    perceptual: 1.0
    gradient_penalty: 10.0
